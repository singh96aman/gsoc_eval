{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T19:03:47.255159Z",
     "start_time": "2019-05-16T19:03:46.609083Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Modules :\n",
    "\n",
    "1. Get all HDF files (+)\n",
    "2. Getting the Unix timestamp of all H5 files and python datetime objects (+)\n",
    " Dataset Name, Parent Group Name, HDF File Name, Singleton Value, Size, Shape, Data type\n",
    "4. Open a HDF File and traverse and add to CSV (+)\n",
    "5. Progress Bar (+)\n",
    "6. Write Error Handling Module\n",
    "7. Caching Module\n",
    "8. Create a dict for a dataset\n",
    "9. Searching Modules\n",
    "10. Query Based Searching\n",
    "11. Searching Error Handling\n",
    "12. Get Attributes\n",
    "13. Load Central CSV Data\n",
    "'''\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import h5py\n",
    "import csv\n",
    "import progressbar\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "file_type = \".h5\"\n",
    "filename_split = \"_\"\n",
    "CERN_timezone = \"Europe/Zurich\"\n",
    "no_of_files_indexed=0\n",
    "count=0\n",
    "\n",
    "fieldnames = ['DatasetName', 'GroupName',\n",
    "              'DatasetAttributes', 'DatasetValue', 'Shape', 'Datatype']\n",
    "\n",
    "\n",
    "def get_all_HDF_files(dir_h5_filepath):\n",
    "    all_files_list = os.listdir(dir_h5_filepath)\n",
    "    all_HDF_files = []\n",
    "    for file in all_files_list:\n",
    "        if file[-3:] == file_type:\n",
    "            all_HDF_files.append(file)\n",
    "    return all_HDF_files\n",
    "\n",
    "\n",
    "def convert_unix_time_to_datetype(filename):\n",
    "    filename_array = filename.split(filename_split)\n",
    "    if len(str(filename_array[0])) == 19:\n",
    "        timestamp = float(filename_array[0])\n",
    "        utc_dt = datetime.utcfromtimestamp(timestamp // 1e9)\n",
    "        cern_tz = pytz.timezone(self.CERN_timezone)\n",
    "        cern_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(cern_tz)\n",
    "        cern_dt = cern_tz.normalize(cern_dt)\n",
    "        return (utc_dt, cern_dt, filename_array[1], filename_array[2])\n",
    "    \n",
    "def init_csv_file(dir_h5_filepath, dir_csv_name, HDF_thread_arr, progress, no_of_threads):\n",
    "    for file in HDF_thread_arr :\n",
    "        csv_file_name = file[:-3]\n",
    "        with open(dir_csv_name+csv_file_name+\".csv\", mode='w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',',\n",
    "                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(fieldnames)\n",
    "            file = h5py.File(dir_h5_filepath+str(file), 'r')\n",
    "            write_into_CSV(file, writer, str(file))\n",
    "            global no_of_files_indexed\n",
    "            no_of_files_indexed = no_of_files_indexed+no_of_threads\n",
    "            progress.update(no_of_files_indexed)\n",
    "            csv_file.close()\n",
    "\n",
    "def init_thread_method(dir_h5_filepath, dir_csv_files, no_of_threads):\n",
    "    HDF_thread_arr, max_size = divide_datasets_for_threads(dir_h5_filepath, no_of_threads)\n",
    "    thread_arr = []\n",
    "    with progressbar.ProgressBar(max_value=max_size, redirect_stdout=True) as progress:\n",
    "        global no_of_files_indexed\n",
    "        progress.update(no_of_files_indexed)\n",
    "        for i in range(0, no_of_threads):\n",
    "            thread_arr.append(threading.Thread(target=init_csv_file(dir_h5_filepath, dir_csv_files, HDF_thread_arr[i], progress, i), name='t'+str(i)))\n",
    "            thread_arr[i].start()\n",
    "        for i in range(0, no_of_threads):\n",
    "            thread_arr[i].join()\n",
    "\n",
    "def init_process_method(dir_h5_filepath, dir_csv_files, no_of_threads):\n",
    "    HDF_thread_arr, max_size = divide_datasets_for_threads(dir_h5_filepath, no_of_threads)\n",
    "    process_arr = []\n",
    "    with progressbar.ProgressBar(max_value=max_size, redirect_stdout=True) as progress:\n",
    "        global no_of_files_indexed\n",
    "        progress.update(no_of_files_indexed)\n",
    "        for i in range(0, no_of_threads):\n",
    "            process_arr.append(multiprocessing.Process(target=init_csv_file, args=(dir_h5_filepath, dir_csv_files, HDF_thread_arr[i], progress, no_of_threads), name='t'+str(i)))\n",
    "            process_arr[i].start()\n",
    "        for i in range(0, no_of_threads):\n",
    "            process_arr[i].join()\n",
    "\n",
    "def get_dataset_attr(file):\n",
    "    DatasetAttributes = {}\n",
    "    for key in file.keys():\n",
    "        DatasetAttributes[key] = str(file[key])\n",
    "    if(len(file.keys()) > 0):\n",
    "        return str(DatasetAttributes)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def divide_datasets_for_threads(dir_h5_filepath, no_of_threads):\n",
    "    all_HDF_files = get_all_HDF_files(dir_h5_filepath)\n",
    "    max_size = len(list(all_HDF_files))\n",
    "    segment_size = int(max_size/no_of_threads)\n",
    "    HDF_thread_arr = []\n",
    "    for i in range(0, max_size, segment_size):\n",
    "        HDF_thread_arr.append(all_HDF_files[i:i+segment_size])\n",
    "    return HDF_thread_arr, max_size\n",
    "\n",
    "\n",
    "def create_central_CSV(file_path, file_path_central):\n",
    "    rows = []\n",
    "    similar_rows = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            if row in rows:\n",
    "                if row not in similar_rows:\n",
    "                    similar_rows.append(row)\n",
    "            else:\n",
    "                rows.append(row)\n",
    "        csvfile.close()\n",
    "    with open(file_path_central, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(similar_rows)\n",
    "        csvfile.close()\n",
    "\n",
    "def get_dataset_value(size, file):\n",
    "    if(int(size) == 0 or int(size) > 1):\n",
    "        return \"\"\n",
    "    else:\n",
    "        if( str(file.dtype) == \"|S1\"):\n",
    "            return ord(file[0])\n",
    "        elif \"S\" in str(file.dtype):\n",
    "            return str(file[0].decode('utf-8'))\n",
    "        else :\n",
    "            return str(file[0])\n",
    "\n",
    "def write_into_CSV(file, writer, main_file_ptr):\n",
    "    if(isinstance(file, h5py.Group)):\n",
    "        for sub in file.keys():\n",
    "            if(isinstance(file[sub], h5py.Dataset)):\n",
    "                DatasetAttributes = get_dataset_attr(file[sub].attrs)\n",
    "                try :\n",
    "                    size = str(file[sub].size)\n",
    "                except :\n",
    "                    size = \"0\"\n",
    "                try :\n",
    "                    dataset_value = get_dataset_value(size, file[sub])\n",
    "                except :\n",
    "                    dataset_value = \"\"\n",
    "                try :\n",
    "                    datatype = str(file[sub].dtype)\n",
    "                except :\n",
    "                    datatype = \"\"\n",
    "                writer.writerow([file[sub].name, file.name, DatasetAttributes, dataset_value, file[sub].shape, datatype])\n",
    "            elif (isinstance(file[sub], h5py.Group)):\n",
    "                write_into_CSV(file[sub], writer, main_file_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
