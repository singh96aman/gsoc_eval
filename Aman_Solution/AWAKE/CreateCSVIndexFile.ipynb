{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T12:43:25.135450Z",
     "start_time": "2019-05-19T12:43:23.458165Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Modules :\n",
    "\n",
    "1. Get all HDF files (+)\n",
    "2. Getting the Unix timestamp of all H5 files and python datetime objects (+)\n",
    " Dataset Name, Parent Group Name, HDF File Name, Singleton Value, Size, Shape, Data type\n",
    "4. Open a HDF File and traverse and add to CSV (+)\n",
    "5. Progress Bar (+)\n",
    "6. Write Error Handling Module\n",
    "7. Caching Module\n",
    "8. Create a dict for a dataset\n",
    "9. Searching Modules\n",
    "10. Query Based Searching\n",
    "11. Searching Error Handling\n",
    "12. Get Attributes\n",
    "13. Load Central CSV Data\n",
    "'''\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import h5py\n",
    "import csv\n",
    "import progressbar\n",
    "import sys\n",
    "import multiprocessing\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "filename_split = \"_\"\n",
    "CERN_timezone = \"Europe/Zurich\"\n",
    "no_of_files_indexed=0\n",
    "count=0\n",
    "\n",
    "fieldnames = ['DatasetName', 'GroupName',\n",
    "              'DatasetAttributes', 'DatasetValue', 'Shape', 'Datatype']\n",
    "\n",
    "\n",
    "def get_all_files(dir_filepath, file_type):\n",
    "    all_files_list = os.listdir(dir_filepath)\n",
    "    all_files = []\n",
    "    for file in all_files_list:\n",
    "        file_fields = file.split(\".\")\n",
    "        if len(file_fields) > 1 :\n",
    "            if str(file_fields[1]) == str(file_type):\n",
    "                all_files.append(file)\n",
    "    return all_files\n",
    "\n",
    "def get_all_CSV_files(dir_h5_filepath):\n",
    "    all_files_list = os.listdir(dir_h5_filepath)\n",
    "    all_HDF_files = []\n",
    "    for file in all_files_list:\n",
    "        if file[-3:] == file_type:\n",
    "            all_HDF_files.append(file)\n",
    "    return all_HDF_files\n",
    "\n",
    "\n",
    "def convert_unix_time_to_datetype(filename):\n",
    "    filename_array = filename.split(filename_split)\n",
    "    if(str(filename_array[0]) == \"optimized\"):\n",
    "        val=1\n",
    "    else:\n",
    "        val=0\n",
    "    if len(str(filename_array[val])) == 19:\n",
    "        timestamp = float(filename_array[val])\n",
    "        utc_dt = datetime.utcfromtimestamp(timestamp // 1e9)\n",
    "        cern_tz = pytz.timezone(CERN_timezone)\n",
    "        cern_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(cern_tz)\n",
    "        cern_dt = cern_tz.normalize(cern_dt)\n",
    "        return (utc_dt, cern_dt, filename_array[1], filename_array[2])\n",
    "    \n",
    "def init_csv_file(dir_h5_filepath, dir_csv_name, HDF_thread_arr, progress, no_of_threads):\n",
    "    for file in HDF_thread_arr :\n",
    "        csv_file_name = file[:-3]\n",
    "        with open(dir_csv_name+csv_file_name+\".csv\", mode='w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',',\n",
    "                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(fieldnames)\n",
    "            file = h5py.File(dir_h5_filepath+str(file), 'r')\n",
    "            write_into_CSV(file, writer, str(file))\n",
    "            global no_of_files_indexed\n",
    "            no_of_files_indexed = no_of_files_indexed+no_of_threads\n",
    "            progress.update(no_of_files_indexed)\n",
    "            csv_file.close()\n",
    "\n",
    "def init_thread_method(dir_h5_filepath, dir_csv_files, no_of_threads):\n",
    "    HDF_thread_arr, max_size = divide_datasets_for_threads(dir_h5_filepath, no_of_threads)\n",
    "    thread_arr = []\n",
    "    with progressbar.ProgressBar(max_value=max_size, redirect_stdout=True) as progress:\n",
    "        global no_of_files_indexed\n",
    "        progress.update(no_of_files_indexed)\n",
    "        for i in range(0, no_of_threads):\n",
    "            thread_arr.append(threading.Thread(target=init_csv_file(dir_h5_filepath, dir_csv_files, HDF_thread_arr[i], progress, i), name='t'+str(i)))\n",
    "            thread_arr[i].start()\n",
    "        for i in range(0, no_of_threads):\n",
    "            thread_arr[i].join()\n",
    "\n",
    "def init_process_method(dir_h5_filepath, dir_csv_files, no_of_threads):\n",
    "    HDF_thread_arr, max_size = divide_datasets_for_threads(dir_h5_filepath, no_of_threads)\n",
    "    process_arr = []\n",
    "    with progressbar.ProgressBar(max_value=max_size, redirect_stdout=True) as progress:\n",
    "        global no_of_files_indexed\n",
    "        progress.update(no_of_files_indexed)\n",
    "        for i in range(0, no_of_threads):\n",
    "            process_arr.append(multiprocessing.Process(target=init_csv_file, args=(dir_h5_filepath, dir_csv_files, HDF_thread_arr[i], progress, no_of_threads), name='t'+str(i)))\n",
    "            process_arr[i].start()\n",
    "        for i in range(0, no_of_threads):\n",
    "            process_arr[i].join()\n",
    "\n",
    "def get_dataset_attr(file):\n",
    "    DatasetAttributes = {}\n",
    "    for key in file.keys():\n",
    "        DatasetAttributes[key] = str(file[key])\n",
    "    if(len(file.keys()) > 0):\n",
    "        return str(DatasetAttributes)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def divide_datasets_for_threads(dir_h5_filepath, no_of_threads):\n",
    "    all_HDF_files = get_all_files(dir_h5_filepath,\"h5\")\n",
    "    max_size = len(list(all_HDF_files))\n",
    "    segment_size = int(max_size/no_of_threads)\n",
    "    HDF_thread_arr = []\n",
    "    for i in range(0, max_size, segment_size):\n",
    "        HDF_thread_arr.append(all_HDF_files[i:i+segment_size])\n",
    "    return HDF_thread_arr, max_size\n",
    "\n",
    "\n",
    "def create_central_CSV(dir_csv_filepath, dir_csv_central_filepath, central_csv_file):\n",
    "    all_CSV_files = get_all_files(dir_csv_filepath, \"csv\")\n",
    "    rows = []\n",
    "    similar_rows = []\n",
    "    for file in all_CSV_files :\n",
    "        with open(dir_csv_filepath+file, 'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            for row in csvreader:\n",
    "                if row in rows:\n",
    "                    if row not in similar_rows and row[3] != \"\":\n",
    "                        similar_rows.append(row)\n",
    "                else:\n",
    "                    rows.append(row)\n",
    "            csvfile.close()\n",
    "        optimize_similar_rows = []\n",
    "        for i in range(0, len(similar_rows)):\n",
    "            found = False\n",
    "            for j in range(0, len(similar_rows)):\n",
    "                if i!=j :\n",
    "                    if similar_rows[i][0] == similar_rows[j][0] :\n",
    "                        found = True\n",
    "                        break\n",
    "            if found == False :\n",
    "                optimize_similar_rows.append(similar_rows[i])\n",
    "    with open(dir_csv_central_filepath+central_csv_file, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(optimize_similar_rows)\n",
    "        csvfile.close()\n",
    "\n",
    "def optimize_csv_files(dir_csv_filepath, dir_csv_central_filepath, central_csv_file):\n",
    "    central_database_name = []\n",
    "    with open(dir_csv_central_filepath+central_csv_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            for row in reader:\n",
    "                central_database_name.append(row)\n",
    "    central_database_name = central_database_name[1:]\n",
    "    all_CSV_files = get_all_files(dir_csv_filepath, \"csv\")\n",
    "    for file in all_CSV_files :\n",
    "        with open(dir_csv_filepath+file, 'r') as csvfile :\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            with open(dir_csv_filepath+\"optimized_\"+file, 'w') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter=',',\n",
    "                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                for row in reader:\n",
    "                    count=0\n",
    "                    for value in central_database_name:\n",
    "                        if(row[0] == value[0]):\n",
    "                            row[1]=count\n",
    "                            row[2]=\"\"\n",
    "                            row[3]=\"\"\n",
    "                            row[4]=\"\"\n",
    "                            row[5]=\"\"\n",
    "                        count = count+1\n",
    "                    writer.writerow(row)\n",
    "        os.remove(dir_csv_filepath+file)\n",
    "\n",
    "def get_dataset_value(size, file):\n",
    "    if(int(size) == 0 or int(size) > 1):\n",
    "        return \"\"\n",
    "    else:\n",
    "        if( str(file.dtype) == \"|S1\"):\n",
    "            return ord(file[0])\n",
    "        elif \"S\" in str(file.dtype):\n",
    "            return str(file[0].decode('utf-8'))\n",
    "        else :\n",
    "            return str(file[0])\n",
    "\n",
    "def write_into_CSV(file, writer, main_file_ptr):\n",
    "    if(isinstance(file, h5py.Group)):\n",
    "        for sub in file.keys():\n",
    "            if(isinstance(file[sub], h5py.Dataset)):\n",
    "                DatasetAttributes = get_dataset_attr(file[sub].attrs)\n",
    "                try :\n",
    "                    size = str(file[sub].size)\n",
    "                except :\n",
    "                    size = \"0\"\n",
    "                try :\n",
    "                    dataset_value = get_dataset_value(size, file[sub])\n",
    "                except :\n",
    "                    dataset_value = \"\"\n",
    "                try :\n",
    "                    datatype = str(file[sub].dtype)\n",
    "                except :\n",
    "                    datatype = \"\"\n",
    "                writer.writerow([file[sub].name, file.name, DatasetAttributes, dataset_value, file[sub].shape, datatype])\n",
    "            elif (isinstance(file[sub], h5py.Group)):\n",
    "                write_into_CSV(file[sub], writer, main_file_ptr)\n",
    "        \n",
    "# Searching Modules !\n",
    "\n",
    "def open_file(file_path):\n",
    "    file = open(file_path, \"r\")\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def get_dataset(filename, parentGroup):\n",
    "    h5File = h5py.File(filename, 'r')\n",
    "    parentGroup = h5File[parentGroup]\n",
    "    dataset = {'DatasetName' : h5File.name}\n",
    "    for file in parentGroup.keys() :\n",
    "        values = parentGroup[file].name.split(\"/\")\n",
    "        val = list(parentGroup[file])\n",
    "        if len(val) == 1 :\n",
    "            dataset.update({values[len(values)-1] : val[0]})\n",
    "        else :\n",
    "            dataset.update({parentGroup.name+\"/\"+str(values[len(values)-1]) : list(val)})\n",
    "    return dataset\n",
    "\n",
    "def search_by_dataset_name(dir_h5_filepath, dir_csv_filepath, dataset_name):\n",
    "    all_csv_files = get_all_files(dir_csv_filepath, \"csv\")\n",
    "    final_solution = []\n",
    "    for file in all_csv_files :\n",
    "        df = open_file(dir_csv_filepath+file)\n",
    "        df = df[df['DatasetName'].str.contains(str(dataset_name))==True]\n",
    "        final_solution.append([file, df])\n",
    "    count = 0\n",
    "    for row in final_solution:\n",
    "        count = count + 1\n",
    "        utc_dt, cern_dt, fileptr1, fileptr2 = convert_unix_time_to_datetype(row[0])\n",
    "        print (str(count)+\" \"+str(cern_dt))\n",
    "    if(len(final_solution) > 0):\n",
    "        val_selected = int(input(\"Enter Which file to load (1,\"+str(count)+\")\"))\n",
    "        main_file_name = final_solution[count-1][0].split(\".\")\n",
    "        filename = main_file_name[0].split(\"_\")\n",
    "        if(filename[0] == \"optimized\"):\n",
    "            filename = filename[1]+\"_\"+filename[2]+\"_\"+filename[3]+\".h5\"\n",
    "        else :\n",
    "            filename = main_file_name[0]+\".h5\"\n",
    "        return get_dataset(dir_h5_filepath+filename, final_solution[count-1][1]['GroupName'].values[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
